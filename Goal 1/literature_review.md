| Title                                | Authors         | Year | Conference/Journal                                             | Contributions                                                                                                                                                                                                 |
|--------------------------------------|-----------------|------|----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Adam: A Method for Stochastic Optimization | Kingma and Ba    | 2014 | ICLR 2015 (International Conference on Learning Representations) | Introduces Adam, an adaptive learning rate optimization algorithm combining momentum and RMSProp. Widely used in training deep neural networks due to its efficiency and low memory requirements.             |
| Attention Is All You Need            | Vaswani et al.  | 2017 | NeurIPS 2017 (originally submitted to arXiv, presented at NeurIPS) | Introduces the Transformer architecture, which relies entirely on self-attention mechanisms, eliminating recurrence. It revolutionized NLP and became the foundation for modern LLMs like BERT and GPT. |
| emg2pose: Zero-shot Full Body Pose Estimation from Sparse EMG | Persad et al.     | 2023 | arXiv preprint (under review)                                    | Proposes a novel framework for estimating full-body pose using only sparse EMG signals without prior pose data. Leverages diffusion models and kinematic priors to reconstruct realistic human motion.         |
| PoseZero: A 3D Human Pose Dataset with Zero Poses | Deng et al.      | 2025 | arXiv preprint                                                  | Introduces a synthetic 3D human pose dataset designed to train and evaluate pose models in zero-pose scenarios. Aims to bridge the gap between motion understanding and pose representation using generative techniques. |
| MoTIF: Motion Token Inference for Pose Forecasting | Liu et al.       | 2024 | ICLR 2024 (OpenReview)                                          | Presents a token-based representation of motion to forecast future human poses. Uses a learned motion vocabulary and transformer architecture to model temporal dynamics effectively with strong results on benchmark datasets. |
| Motion-X: A Large-Scale 3D Motion Understanding Benchmark | Zhang et al.     | 2024 | ICLR 2024 (OpenReview)                                          | Introduces Motion-X, a large-scale 3D motion dataset and benchmark for action recognition, motion prediction, and synthesis. Includes over 1,000 hours of data, setting a new standard for evaluating 3D motion understanding models. |
| Prompting Paradigms for Language Models: A New Stage for NLU | Liu et al.       | 2024 | ACL 2024 (Tutorial)                                             | Provides a comprehensive tutorial on prompting techniques for LLMs—covering zero-shot, few-shot, chain-of-thought, self-consistency, and more—framing prompting as a paradigm shift in natural language understanding. |
| Skelformer: 3D Human Motion Forecasting with Self-supervised Skeleton Representation | Zheng et al.     | 2024 | arXiv preprint                                                  | Proposes Skelformer, a Transformer-based architecture using self-supervised contrastive learning to encode skeleton dynamics. Achieves strong performance on standard motion forecasting benchmarks.              |
| Denoise and Diffuse: A Two-Stage Approach for Stochastic 3D Human Motion Prediction | Zhang et al.     | 2025 | arXiv preprint                                                  | Proposes a two-stage architecture combining denoising and diffusion for realistic and diverse 3D motion prediction. The denoising refines noisy pose inputs, while the diffusion model generates plausible future trajectories. |
